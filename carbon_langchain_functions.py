# -*- coding: utf-8 -*-
"""Carbon Langchain_Functions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15S3HamYQh2utG_M8ohMH5Crfw6P4RrtJ

Simple langchain example using Google's Generate AI API.

You'll need an API key from Generative AI Studio.

Fist we install dependencies to use langserv.
"""

# !pip install langchain langchain-cli "langserve[all]" google.generativeai langchain-google-genai pandas numexpr  langchain-openai

# basic langchain imports
from dotenv import load_dotenv
from langchain.chains import ConversationalRetrievalChain
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts import MessagesPlaceholder
from langchain.chains.conversation.memory import ConversationBufferWindowMemory

# Import things for functions
from langchain.pydantic_v1 import BaseModel, Field
from langchain.tools import BaseTool, StructuredTool, tool
# agents
from langchain.agents import initialize_agent, AgentExecutor, create_structured_chat_agent

from langchain.globals import set_verbose
from langchain.globals import set_debug
import pandas as pd
import json
import os
from dotenv import load_dotenv


USE_GOOGLE_GEMINI = True

load_dotenv()
# Get the key from https://aistudio.google.com
google_api_key =  os.getenv('GOOGLE_API_KEY')
# Get the key from https://platform.openai.com/api-keys
openai_api_key = os.getenv('OPENAI_API_KEY')

url = 'https://raw.githubusercontent.com/GoogleCloudPlatform/region-carbon-info/main/data/yearly/2022.csv'
df_cfe = pd.read_csv(url)
print('cfe:')
df_cfe.rename(columns={'Google Cloud Region': 'region', 'Google CFE': 'cfe'}, inplace=True)


url = 'https://raw.githubusercontent.com/rcleveng/notebooks/main/gcp_latency_20240613.csv'
df_latency = pd.read_csv(url)
print('latency:')
print(df_latency.head(5))

import numexpr as ne
from typing import Tuple, Union, Dict

SYSTEM = '''Respond to the human as helpfully and accurately as possible. You have access to the following tools:

{tools}

Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).

Valid "action" values: "Final Answer" or {tool_names}

Provide only ONE action per $JSON_BLOB, as shown:

```

{{

  "action": $TOOL_NAME,

  "action_input": $INPUT

}}

```

Follow this format:

Question: input question to answer

Thought: consider previous and subsequent steps

Action:

```

$JSON_BLOB

```

Observation: action result

... (repeat Thought/Action/Observation N times)

Thought: I know what to respond

Action:

```

{{

  "action": "Final Answer",

  "action_input": "Final response to human"

}}

Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation
'''


HUMAN = '''{input}

{agent_scratchpad}

(reminder to respond in a JSON blob no matter what)'''

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", SYSTEM),
        MessagesPlaceholder("chat_history", optional=True),
        ("human", HUMAN),
    ]
)

if USE_GOOGLE_GEMINI:
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=google_api_key)
    # Work around gemini 1.0 limitation and general silliness
    llm.convert_system_message_to_human = True
else:
    llm = ChatOpenAI(
        model="gpt-4o",
        temperature=0,
        max_tokens=None,
        timeout=None,
        max_retries=10,
        api_key=openai_api_key,
    )


class GcpRegionSchema(BaseModel):
    region: str = Field(description="name of the Google Cloud Region", default="")

class NoParameterInput(BaseModel):
    pass

class CfeAllTool(BaseTool):
    name = "cfe_all"
    description = "Fetch the carbon free energy (CFE) percent for every Google Cloud regions. No parameter needed from input"
    args_schema = NoParameterInput

    def is_single_input(self) -> bool:
      return True

    def _to_args_and_kwargs(self, tool_input: Union[str, Dict]) -> Tuple[Tuple, Dict]:
        print("_to_args_and_kwargs")
        return (), {}

    def _run(self):
        print("executing cfe-all-tool")
        print(df_cfe.head(5))
        df_cfe.rename(columns={'Google Cloud Region': 'region', 'Google CFE': 'cfe'}, inplace=True)
        return df_cfe[['region', 'cfe']].to_string(
            columns=['region', 'cfe'], 
            index=False, 
            index_names=False, 
            header=False,
            )

    def _arun(self):
        raise NotImplementedError("does not support async")

@tool("latency-tool", args_schema=GcpRegionSchema, return_direct=False)
def latency(region) -> str:
  """fetch the network latency from a given google cloud region to all other regions"""
  print(f"executing latency-tool for {region}")
  ms_formatter = lambda x: '%4.2f ms' % x

  return df_latency.loc[df_latency['sending_region'] == region].to_string(
      columns=['receiving_region', 'milliseconds'], 
      formatters={'milliseconds': ms_formatter},
      index=False, 
      index_names=False, 
      header=False
      )

cfe_all = CfeAllTool()
tools = [cfe_all, latency]

# initialize conversational memory
conversational_memory = ConversationBufferWindowMemory(
        memory_key='chat_history',
        k=5,
        return_messages=True)

# initialize agent with tools
agent = create_structured_chat_agent(llm, tools, prompt)

agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=False,
    handle_parsing_errors=True,
    memory=conversational_memory,
    max_iterations=50,
    early_stopping_method='generate',
)

#set_debug(True)
while True:
    conversational_memory.clear()
    query = input("Input: ")
    chat_history = conversational_memory.buffer_as_messages
    out1 = agent_executor.invoke({
        "input": query,
        "chat_history": chat_history,
    })
    print(out1['output'])
    print("\n\n\n")
